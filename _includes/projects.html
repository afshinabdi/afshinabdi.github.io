<section id="projects">
<div class="container">
  <h3><strong>Projects</strong></h3>
  <hr style="height:2px; visibility:show; background:gray; margin-top:0px" />
  <div class="panel panel-default">
    <div class="panel-body">
	<div>
		<h3>Distributed Deep Learning</h3>
		<p>
			One of the main challenges in distributed training is the communication cost due to the transmission of the parameters or stochastic gradients (SGs) of the deep model for synchronization across processing nodes/workers. To reduce the communication bottleneck in distributed training, I have approached the problem from three different perspectives: <em>compressive sampling, matrix decomposition, and Information theory and the Central Estimation Officer (CEO) problem</em>.
		</p>

		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<a href="{{ site.baseurl }}DistributedTraining-QCS/">Quantized Compressive Sampling</a>
		</h5>
		<div class="row">
			<div class="col-sm-8" align="justify">
			<p>
				The goal is achieving an arbitrarily large unbiased compression of SG while ensuring the mean squared
				error (MSE) is low. Inspired by the design of structured mixing matrices in compressed sensing, 
				I developed Quantized Compressive Sampling (QCS) and showed that it can achieve orders of magnitude 
				smaller MSE compared to the state-ofthe-art unbiased compression techniques, resulting in superior
				convergence rate.
			</p>
			</div>
			<div class="col-sm-4" align="center">
				<img width=240 src="{{ site.baseurl }}static/img/projects/quantizedCS.png">
			</div>
		</div>

		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<a href="{{ site.baseurl }}DistributedTraining-ISGQ/">Indirect Stochastic Gradient Compression</a>
		</h5>
		<div class="row">
			<div class="col-sm-8" align="justify">
			<p>
				One way of compressing the stochastic gradients in distributed learning is via
				low-rank matrix decomposition and then quantizing the factorized terms. However, 
				naively pursuing such an approach is costly, both 
				in terms of the computations and the resulting error during training. I used the 
				factorization inherently provided during the backpropagation algorithm and developed 
				unbiased quantization techniques for compression.
			</p>
			</div>
			<div class="col-sm-4" align="center">
				<img width=240 src="{{ site.baseurl }}static/img/projects/isgq.png">
			</div>
		</div>

		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<a href="{{ site.baseurl }}DistributedTraining-CEO/">Distributed Deep Learning via CEO</a>
		</h5>
		<div class="row">
			<div class="col-sm-8" align="justify">
			<p>
				Distributed learning can be framed as a CEO problem; the computations at each worker 
				can be considered as a noisy observation of the true update (or gradient), and the 
				objective of distributed learning would be reliable estimation of the true update with minimum
				communication from workers.
			</p>
			</div>
			<div class="col-sm-4" align="center">
				<img width=240 src="{{ site.baseurl }}static/img/projects/ceo.png">
			</div>
		</div>
	</div>

	<hr style="height:2px; visibility:show;" />
	<!--hr style="height:5px; visibility:show;" /-->
<!-------------------------------------------------------------------------------------->
	<div>
		<h3>Federated Learning over Wireless Edge Networks</h3>
		<p>
			Transmitting stochastic gradients (SG)
			or deep model’s parameters over a limited-bandwidth wireless
			channel can incur large training latency and excessive power
			consumption. Hence, data compressing is often used to reduce
			the communication overhead. However, efficient communication
			requires the compression algorithm to satisfy the constraints
			imposed by the network, such as unreliable transmission
			and idle nodes in the edge network, limited transmission power,
			and the requirement to preserve data privacy.
		</p>

		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<a href="{{ site.baseurl }}FederatedLearning-RLC/">Analog Compression</a>
		</h5>
		<div class="row">
			<div class="col-sm-8" align="justify">
			<p>
				Efficient communication for federated learning over multiple-access channels 
				requires satisfying the constraints imposed by the communication medium and taking 
				advantage of its characteristics, especially, (i) The MAC channel can naturally 
				compute weighted average of the transmitted values, and 
				(ii) edge-node’s private information should not leak to the edge router. 
				These requirements enforce a Homomorphic property on the encoders. 
				Analog compression uses random partial Hadamard matrices to provide a 
				low-MSE unbiased SG compression, while satisfying the requirements of the FL over wireless MAC.
			</p>
			</div>
			<div class="col-sm-4" align="center">
				<img width=240 src="{{ site.baseurl }}static/img/projects/analog_fl_mac.png">
			</div>
		</div>

	</div>

	<hr style="height:1px; visibility:show; background:gray;" />
<!-------------------------------------------------------------------------------------->
	<div>
		<h3>Model Compression</h3>
		<p>
			While the complexity of modern deep neural networks allows them to
			learn complicated tasks, the large number of parameters makes them
			prone to over-fitting, which adversely affects the accuracy and variance of
			deep neural networks. Moreover, computational complexity and memory
			footprint restrict the deployment of such deep models in many end-user
			devices. Hence, model reduction is a highly desirable process for deep
			neural networks.
		</p>

		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<a href="https://dnntoolbox.github.io/Net-Trim/">Net-Trim: Convex Pruning of Neural Networks</a>
		</h5>
		<div class="row">
			<div class="col-sm-8" align="justify">
			<p>
				A (deep) neural network can be considered as a combination of processing blocks, 
				where each block is composed of a linear operator (representable by a matrix), 
				and a nonlinear unit. Net-Trim is a post-processing module, which applies to a 
				network that is already trained, and simplifies the network architecture by pruning 
				each block. Net-Trim reprocesses and prunes the network block by block such that 
				the response of each block to the input data remains almost intact and similar to 
				the original model.
			</p>
			</div>
			<div class="col-sm-4" align="center">
				<img width=240 height=180 src="{{ site.baseurl }}static/img/projects/nettrim.png">
			</div>
		</div>

	</div>

	<hr style="height:1px; visibility:show; background:gray;" />
<!-------------------------------------------------------------------------------------->
	<!--/div>
		<h5>
			<i class="fa fa-globe"></i>&nbsp;&nbsp;<strong><a href="#">Project 4</a></strong>
			- Vim an case vidit sententiae, est at euismod laboramus efficiantur. Ei sit brute lorem,
			ea eam timeam viderer aliquid, cu integre omittam moderatius quo.
		</h5>
	<hr style="height:10px; visibility:show;" />
		<h5>
			<i class="fa fa-cog"></i>&nbsp;&nbsp;<strong><a href="#">Project 6</a></strong>
			- Inermis recusabo elaboraret mea et, dicat neglegentur vim te. Nec et sanctus scriptorem,
			usu ex sapientem gubergren. Tamquam admodum ei usu.
		</h5>

    </div-->
</div>
</section>
